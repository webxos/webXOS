<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>edTV: CHANNEL 7</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link href="https://fonts.googleapis.com/css2?family=Press+Start+2P&display=swap" rel="stylesheet">
<style>
  *{margin:0;padding:0;box-sizing:border-box;font-family:'Press Start 2P',cursive;image-rendering:pixelated}
  body{background:#000;color:#0f0;overflow:hidden;height:100dvh;user-select:none;touch-action:none}
  canvas{display:block}
  
  /* Header */
  .banner{
    position:absolute;
    top:0;
    left:0;
    right:0;
    background:rgba(0,20,0,0.95);
    border-bottom:2px solid #0f0;
    padding:8px;
    font-size:11px;
    text-align:center;
    z-index:10;
    text-shadow:0 0 10px #0f0;
  }
  
  /* Status bar */
  .status{
    position:absolute;
    bottom:8px;
    left:8px;
    right:8px;
    display:flex;
    gap:8px;
    z-index:10;
  }
  .ind{
    padding:4px 8px;
    font-size:8px;
    background:rgba(0,50,0,0.9);
    border:1px solid #0f0;
    border-radius:4px;
    flex:1;
    text-align:center;
  }
  .locked{background:rgba(0,255,0,0.9);box-shadow:0 0 12px #0f0;animation:p 1.5s infinite}
  @keyframes p{0%,100%{opacity:1}50%{opacity:0.6}}
  
  /* Control Panel */
  .control-panel {
    position: absolute;
    top: 50px;
    right: 8px;
    width: 160px;
    background: rgba(0, 20, 0, 0.85);
    border: 2px solid #0f0;
    border-radius: 8px;
    padding: 12px;
    display: flex;
    flex-direction: column;
    gap: 12px;
    z-index: 10;
  }
  
  .control-group {
    display: flex;
    flex-direction: column;
    gap: 4px;
  }
  
  .control-label {
    font-size: 8px;
    color: #0f0;
    margin-bottom: 4px;
  }
  
  .slider {
    -webkit-appearance: none;
    width: 100%;
    height: 12px;
    background: rgba(0, 50, 0, 0.9);
    border-radius: 6px;
    outline: none;
    border: 1px solid #0f0;
  }
  
  .slider::-webkit-slider-thumb {
    -webkit-appearance: none;
    width: 16px;
    height: 16px;
    border-radius: 50%;
    background: #0f0;
    cursor: pointer;
    box-shadow: 0 0 5px #0f0;
  }
  
  .value-display {
    font-size: 8px;
    text-align: center;
    margin-top: 4px;
  }
  
  /* Buttons */
  .camera-button, .util-button {
    background: rgba(0, 50, 0, 0.9);
    border: 2px solid #0f0;
    border-radius: 8px;
    color: #0f0;
    font-size: 10px;
    padding: 8px;
    cursor: pointer;
    text-align: center;
    transition: all 0.3s;
    box-shadow: 0 0 5px rgba(0, 255, 0, 0.3);
  }
  .camera-button:active, .util-button:active {
    background: rgba(0, 255, 0, 0.3);
    transform: scale(0.95);
  }

  .file-input {
    font-size: 8px;
    color: #0f0;
    border: 1px dashed #0f0;
    padding: 6px;
    border-radius: 6px;
    background: rgba(0, 20, 0, 0.6);
  }
  
  /* Mobile-specific adjustments */
  @media (max-width: 768px) {
    .banner { font-size: 9px; padding: 6px; }
    .control-panel { width: 150px; top: 40px; right: 6px; padding: 10px; }
    .ind { font-size: 7px; padding: 3px 6px; }
    .camera-button, .util-button { font-size: 9px; padding: 6px; }
  }
  
  @media (max-width: 480px) {
    .control-panel { width: 140px; padding: 8px; }
    .control-label, .value-display { font-size: 7px; }
    .slider { height: 10px; }
    .slider::-webkit-slider-thumb { width: 14px; height: 14px; }
  }

  /* Error message */
  .error-message {
    position: absolute;
    top: 50%;
    left: 50%;
    transform: translate(-50%, -50%);
    background: rgba(255, 0, 0, 0.8);
    color: white;
    padding: 12px;
    border-radius: 8px;
    font-size: 10px;
    text-align: center;
    z-index: 20;
    display: none;
    max-width: 300px;
  }
  
  /* Resolution info */
  .resolution-info {
    position: absolute;
    top: 50px;
    left: 8px;
    background: rgba(0, 20, 0, 0.85);
    border: 2px solid #0f0;
    border-radius: 8px;
    padding: 8px;
    font-size: 8px;
    z-index: 10;
    min-width: 160px;
  }
  
  .error-dismiss {
    background: rgba(255, 255, 255, 0.2);
    border: 1px solid white;
    color: white;
    font-size: 8px;
    padding: 4px 8px;
    margin-top: 8px;
    cursor: pointer;
    border-radius: 4px;
  }

  .preview {
    position: absolute;
    top: 120px;
    left: 8px;
    background: rgba(0, 20, 0, 0.85);
    border: 2px solid #0f0;
    border-radius: 8px;
    padding: 8px;
    font-size: 8px;
    z-index: 10;
  }
  .preview canvas {
    border: 1px solid #0f0;
    width: 160px;
    height: 160px;
  }
</style>
</head>
<body>
<div class="banner">edTV: CHANNEL 7</div>

<canvas id="c"></canvas>

<div class="resolution-info" id="resolutionInfo">
  OUTPUT: 200x200 CLOUD â€¢ STATIC GRID: 200x200
</div>

<div class="control-panel">
  <div class="control-group">
    <div class="control-label">DEPTH (LIVE)</div>
    <input type="range" min="0.1" max="1.5" step="0.1" value="0.8" class="slider" id="depthScale">
    <div class="value-display" id="depthValue">0.8</div>
  </div>
  
  <div class="control-group">
    <div class="control-label">POINT SIZE</div>
    <input type="range" min="0.001" max="0.02" step="0.001" value="0.003" class="slider" id="pointSize">
    <div class="value-display" id="sizeValue">0.003</div>
  </div>

  <div class="control-group">
    <div class="control-label">STATIC BLEND</div>
    <input type="range" min="0" max="1" step="0.05" value="1" class="slider" id="blendWeight">
    <div class="value-display" id="blendValue">1.00</div>
  </div>

  <button class="camera-button" id="syncButton">SYNC</button>
  <button class="camera-button" id="resetButton">RESET</button>

  <div class="control-group">
    <div class="control-label">UPLOAD IMAGE</div>
    <input type="file" id="imgUpload" accept="image/*" class="file-input">
  </div>

  <button class="util-button" id="toggleStatic">TOGGLE STATIC</button>
  <button class="util-button" id="clearStatic">CLEAR STATIC</button>
</div>

<div class="status">
  <div class="ind" id="status">DA3: READY</div>
</div>

<div class="error-message" id="errorMessage">
  <div id="errorText"></div>
  <button class="error-dismiss" id="errorDismiss">DISMISS</button>
</div>

<div class="preview" id="previewBox" style="display:none;">
  <div>STATIC PREVIEW</div>
  <canvas id="imgCanvas" width="200" height="200"></canvas>
  <div style="margin-top:6px;">DEPTH / NORMAL</div>
  <canvas id="mapCanvas" width="200" height="200"></canvas>
</div>

<script type="module">
import * as THREE from 'https://cdn.jsdelivr.net/npm/three@0.158.0/build/three.module.js';
import '@tensorflow/tfjs-backend-webgl';
import * as faceLandmarksDetection from 'https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection@2.0.1/dist/face-landmarks-detection.js';

// Optional Poisson reconstruction (if available)
// Note: If CDN fails, code falls back to depth-to-points without Poisson.
let Poisson;
try {
  Poisson = await import('https://cdn.jsdelivr.net/npm/poisson-recon-js@latest/dist/poisson-recon.js');
} catch (e) {
  Poisson = null;
  console.log('Poisson recon module not available, using depth-based point cloud.');
}

// Transformers.js depth-estimation
let transformers;
try {
  // Load UMD build
  // eslint-disable-next-line no-undef
  transformers = await import('https://cdn.jsdelivr.net/npm/@xenova/transformers@latest/dist/transformers.min.js');
} catch (e) {
  transformers = null;
  console.log('Transformers.js failed to load.');
}

// Core elements
const canvas = document.getElementById('c');
const renderer = new THREE.WebGLRenderer({canvas, antialias:false, alpha:true});
renderer.setPixelRatio(Math.min(devicePixelRatio, 1.5));
renderer.setSize(innerWidth, innerHeight);

const scene = new THREE.Scene();
scene.background = new THREE.Color(0x000000);

const camera = new THREE.PerspectiveCamera(45, innerWidth/innerHeight, 0.1, 10);
camera.position.set(0,0,0.8);

const light = new THREE.HemisphereLight(0x00ff88, 0x0088ff, 1.2);
scene.add(light);

// App state
let detector, pointsGeo, pointsMat, points;
let staticGeo, staticMat, staticPoints;
let currentPoints = new Float32Array(478*3);
let depthScale = 0.8;
let pointSize = 0.003;
let blendWeight = 1.0;
let lastFaceTime = 0;
const FACE_TIMEOUT = 1000;

let staticEnabled = false;

// UI elements
const depthScaleSlider = document.getElementById('depthScale');
const pointSizeSlider = document.getElementById('pointSize');
const depthValueDisplay = document.getElementById('depthValue');
const sizeValueDisplay = document.getElementById('sizeValue');
const blendSlider = document.getElementById('blendWeight');
const blendValueDisplay = document.getElementById('blendValue');
const syncButton = document.getElementById('syncButton');
const resetButton = document.getElementById('resetButton');
const errorMessage = document.getElementById('errorMessage');
const errorText = document.getElementById('errorText');
const errorDismiss = document.getElementById('errorDismiss');
const statusElement = document.getElementById('status');
const imgUpload = document.getElementById('imgUpload');
const toggleStaticBtn = document.getElementById('toggleStatic');
const clearStaticBtn = document.getElementById('clearStatic');

const previewBox = document.getElementById('previewBox');
const imgCanvas = document.getElementById('imgCanvas');
const mapCanvas = document.getElementById('mapCanvas');
const imgCtx = imgCanvas.getContext('2d', { willReadFrequently: true });
const mapCtx = mapCanvas.getContext('2d', { willReadFrequently: true });

// Performance tracking
let needsBufferUpdate = false;
let detectionInProgress = false;
let consecutiveDetectionErrors = 0;
const MAX_CONSECUTIVE_ERRORS = 3;

// Processing canvas
let processedCanvas = null;
let processedCtx = null;
let video = null;

// Update slider displays
depthScaleSlider.addEventListener('input', (e) => {
  depthScale = parseFloat(e.target.value);
  depthValueDisplay.textContent = depthScale.toFixed(1);
});
pointSizeSlider.addEventListener('input', (e) => {
  pointSize = parseFloat(e.target.value);
  sizeValueDisplay.textContent = pointSize.toFixed(3);
  if (pointsMat) {
    pointsMat.size = pointSize;
    pointsMat.needsUpdate = true;
  }
  if (staticMat) {
    staticMat.size = pointSize;
    staticMat.needsUpdate = true;
  }
});
blendSlider.addEventListener('input', (e) => {
  blendWeight = parseFloat(e.target.value);
  blendValueDisplay.textContent = blendWeight.toFixed(2);
  if (staticPoints) {
    staticPoints.visible = staticEnabled && blendWeight > 0.01;
    staticPoints.material.opacity = Math.min(1, Math.max(0.1, blendWeight));
    staticPoints.material.transparent = staticPoints.material.opacity < 1;
  }
});

// Error handling
errorDismiss.addEventListener('click', () => hideError());
function showError(message) {
  errorText.textContent = message;
  errorMessage.style.display = 'block';
}
function hideError() { errorMessage.style.display = 'none'; }

// SYNC button - initialize camera and start detection
syncButton.addEventListener('click', async () => {
  hideError();
  statusElement.textContent = 'DA3: SYNCING...';
  syncButton.disabled = true;
  try {
    await initCamera();
    if (video) {
      await initFaceDetection();
      statusElement.textContent = 'DA3: ONLINE';
      statusElement.classList.add('locked');
      requestAnimationFrame(loop);
    }
  } catch (error) {
    console.error('Sync error:', error);
    showError('Failed to sync. Check camera permissions.');
    statusElement.textContent = 'DA3: SYNC FAILED';
    syncButton.disabled = false;
  }
});

// RESET button - stop everything and clear
resetButton.addEventListener('click', () => {
  hideError();
  if (video && video.srcObject) {
    video.srcObject.getTracks().forEach(track => track.stop());
    video.srcObject = null;
    video.remove();
    video = null;
  }
  if (points) points.visible = false;
  if (staticPoints) staticPoints.visible = false;
  statusElement.textContent = 'DA3: READY';
  statusElement.classList.remove('locked');
  consecutiveDetectionErrors = 0;
  syncButton.disabled = false;
});

// Toggle / clear static
toggleStaticBtn.addEventListener('click', () => {
  staticEnabled = !staticEnabled;
  if (staticPoints) staticPoints.visible = staticEnabled && blendWeight > 0.01;
  toggleStaticBtn.textContent = staticEnabled ? 'STATIC: ON' : 'STATIC: OFF';
});
clearStaticBtn.addEventListener('click', () => {
  if (staticPoints) staticPoints.visible = false;
  staticEnabled = false;
  toggleStaticBtn.textContent = 'TOGGLE STATIC';
  previewBox.style.display = 'none';
});

// Initialize Three.js components
pointsMat = new THREE.PointsMaterial({ color: 0x00ff88, size: pointSize, sizeAttenuation: true });
pointsGeo = new THREE.BufferGeometry();
pointsGeo.setAttribute('position', new THREE.BufferAttribute(currentPoints, 3));
points = new THREE.Points(pointsGeo, pointsMat);
scene.add(points);

// Static points setup
staticMat = new THREE.PointsMaterial({ color: 0x88ff00, size: pointSize, sizeAttenuation: true, opacity: 1.0 });
staticGeo = new THREE.BufferGeometry();
staticPoints = new THREE.Points(staticGeo, staticMat);
staticPoints.visible = false;
scene.add(staticPoints);

async function initCamera() {
  if (video && video.srcObject) {
    video.srcObject.getTracks().forEach(track => track.stop());
    video.srcObject = null;
    video.remove();
  }
  const constraints = {
    video: { facingMode: 'user', width: { ideal: 320 }, height: { ideal: 240 } }
  };
  try {
    video = document.createElement('video');
    video.width = 320; video.height = 240; video.autoplay = true; video.playsInline = true;
    const stream = await navigator.mediaDevices.getUserMedia(constraints);
    video.srcObject = stream;
    return new Promise((resolve) => { video.onloadedmetadata = () => resolve(video); });
  } catch (err) {
    console.error('Camera error:', err);
    showError('Camera access denied. Please allow camera permissions.');
    return null;
  }
}

async function initFaceDetection() {
  try {
    detector = await faceLandmarksDetection.createDetector(
      faceLandmarksDetection.SupportedModels.MediaPipeFaceMesh,
      { runtime: 'tfjs', refineLandmarks: true, maxFaces: 1 }
    );
    if (!processedCanvas) {
      processedCanvas = document.createElement('canvas');
      processedCanvas.width = 200;
      processedCanvas.height = 200;
      processedCtx = processedCanvas.getContext('2d', { willReadFrequently: true });
      processedCtx.imageSmoothingEnabled = false;
    }
    return true;
  } catch (error) {
    console.error('Face detection init error:', error);
    showError('Face detection failed to load.');
    return false;
  }
}

function clearPoints() {
  points.visible = false;
  needsBufferUpdate = false;
}

// Process video frame to 200x200
function processVideoFrame() {
  if (!processedCtx || !video) return false;
  try {
    processedCtx.drawImage(video, 0, 0, 200, 200);
    return true;
  } catch (error) {
    console.error('Frame processing error:', error);
    return false;
  }
}

async function processFaceDetection() {
  if (!detector || !video || detectionInProgress) return;
  detectionInProgress = true;
  try {
    if (!processVideoFrame()) throw new Error('Frame processing failed');
    const faces = await detector.estimateFaces(processedCanvas, { flipHorizontal: false });
    consecutiveDetectionErrors = 0;
    hideError();
    if (faces.length > 0) {
      lastFaceTime = Date.now();
      const landmarks = faces[0].keypoints;
      const numPoints = landmarks.length;
      if (currentPoints.length !== numPoints * 3) {
        if (pointsGeo) pointsGeo.dispose();
        currentPoints = new Float32Array(numPoints * 3);
        pointsGeo = new THREE.BufferGeometry();
        pointsGeo.setAttribute('position', new THREE.BufferAttribute(currentPoints, 3));
        points.geometry = pointsGeo;
      }
      const canvasWidth = processedCanvas.width;
      const canvasHeight = processedCanvas.height;
      for (let i = 0; i < numPoints; i++) {
        const p = landmarks[i];
        currentPoints[i*3]   = (p.x / canvasWidth - 0.5) * 0.6;
        currentPoints[i*3+1] = -(p.y / canvasHeight - 0.5) * 0.6;
        currentPoints[i*3+2] = (p.z * 0.001 || 0) * depthScale;
      }
      points.visible = true;
      needsBufferUpdate = true;
    } else {
      if (Date.now() - lastFaceTime > FACE_TIMEOUT) clearPoints();
    }
  } catch (error) {
    console.error('Face detection error:', error);
    consecutiveDetectionErrors++;
    if (consecutiveDetectionErrors >= MAX_CONSECUTIVE_ERRORS) {
      showError('Detection unstable. Reinitializing detector...');
      consecutiveDetectionErrors = 0;
      try { await initFaceDetection(); hideError(); } catch (retryError) {
        console.error('Detector reinitialization failed:', retryError);
      }
    }
  } finally {
    detectionInProgress = false;
  }
}

// Depth estimation pipeline init
let depthPipeline = null;
async function getDepthPipeline() {
  if (!transformers) return null;
  if (depthPipeline) return depthPipeline;
  try {
    const { pipeline } = transformers;
    // Smaller model for browser perf; if not available, catch and return null
    depthPipeline = await pipeline('depth-estimation', 'Xenova/depth-anything-small');
    return depthPipeline;
  } catch (e) {
    console.log('Depth pipeline load failed:', e);
    return null;
  }
}

// Convert uploaded image to depth map (Float32Array), then normals (Uint8ClampedArray for preview)
function computeNormalsFromDepth(depth, w, h) {
  const normals = new Uint8ClampedArray(w * h * 4);
  // Sobel kernels on depth
  const get = (x,y) => depth[Math.min(h-1, Math.max(0,y)) * w + Math.min(w-1, Math.max(0,x))];
  for (let y = 0; y < h; y++) {
    for (let x = 0; x < w; x++) {
      const xm1 = x-1, xp1 = x+1, ym1 = y-1, yp1 = y+1;
      const gx = (-get(xm1, ym1) - 2*get(xm1, y) - get(xm1, yp1) + get(xp1, ym1) + 2*get(xp1, y) + get(xp1, yp1));
      const gy = (-get(xm1, ym1) - 2*get(x, ym1) - get(xp1, ym1) + get(xm1, yp1) + 2*get(x, yp1) + get(xp1, yp1));
      let nx = -gx, ny = -gy, nz = 1.0;
      const len = Math.max(1e-6, Math.sqrt(nx*nx + ny*ny + nz*nz));
      nx /= len; ny /= len; nz /= len;
      const i = (y*w + x) * 4;
      normals[i]   = Math.round((nx * 0.5 + 0.5) * 255);
      normals[i+1] = Math.round((ny * 0.5 + 0.5) * 255);
      normals[i+2] = Math.round((nz * 0.5 + 0.5) * 255);
      normals[i+3] = 255;
    }
  }
  return normals;
}

// Reconstruct static point cloud from depth (optionally Poisson if available)
function reconstructStaticPoints(depth, w, h, normalPreview) {
  const scale = 0.6; // fit to viewport like live points
  const zScale = 0.4; // static depth scale
  const step = 2; // sample grid spacing

  const countX = Math.floor(w / step);
  const countY = Math.floor(h / step);
  const total = countX * countY;
  const positions = new Float32Array(total * 3);
  let ptr = 0;

  for (let yy = 0; yy < h; yy += step) {
    for (let xx = 0; xx < w; xx += step) {
      const idx = yy * w + xx;
      const d = depth[idx]; // [0,1] approximate
      const nx = (xx / w - 0.5) * scale;
      const ny = -((yy / h - 0.5) * scale);
      const nz = (1.0 - d) * zScale; // inverse: nearer = higher z
      positions[ptr++] = nx;
      positions[ptr++] = ny;
      positions[ptr++] = nz;
    }
  }

  // If Poisson recon library is available, attempt densification (optional)
  if (Poisson && Poisson.reconstructSurface) {
    try {
      // Hypothetical API: reconstructSurface(points, normals) -> dense points Float32Array
      // Map normal preview RGBA to normals [-1,1]
      const normals = [];
      for (let yy = 0; yy < h; yy += step) {
        for (let xx = 0; xx < w; xx += step) {
          const i = ((yy * w) + xx) * 4;
          const nx = (normalPreview[i] / 255) * 2 - 1;
          const ny = (normalPreview[i+1] / 255) * 2 - 1;
          const nz = (normalPreview[i+2] / 255) * 2 - 1;
          normals.push(nx, ny, nz);
        }
      }
      const dense = Poisson.reconstructSurface(positions, new Float32Array(normals), { depth: 8, samples: 2 });
      if (dense && dense.length > 0) return dense;
    } catch (e) {
      console.log('Poisson reconstruction failed, using grid points.');
    }
  }
  return positions;
}

// Handle image upload -> depth -> normals -> static points
imgUpload.addEventListener('change', async (e) => {
  const file = e.target.files?.[0];
  if (!file) return;
  const url = URL.createObjectURL(file);
  const img = new Image();
  img.onload = async () => {
    // Draw image to preview
    imgCtx.clearRect(0,0,200,200);
    imgCtx.drawImage(img, 0, 0, 200, 200);
    previewBox.style.display = 'block';

    // Depth estimation
    let depthData = null;
    try {
      const pipe = await getDepthPipeline();
      if (pipe) {
        const result = await pipe(img);
        // result.predicted_depth is a tensor HxW, normalize to [0,1]
        const arr = await result.predicted_depth.data();
        const w = result.predicted_depth.dims[1];
        const h = result.predicted_depth.dims[0];
        // Resize to 200x200
        const tmp = document.createElement('canvas');
        tmp.width = w; tmp.height = h;
        const tctx = tmp.getContext('2d');
        // Put depth as image by normalizing
        const min = Math.min(...arr);
        const max = Math.max(...arr);
        const imgData = tctx.createImageData(w, h);
        for (let i = 0; i < arr.length; i++) {
          const v = (arr[i] - min) / Math.max(1e-6, (max - min));
          const ii = i * 4;
          const c = Math.round(v * 255);
          imgData.data[ii] = c; imgData.data[ii+1] = c; imgData.data[ii+2] = c; imgData.data[ii+3] = 255;
        }
        tctx.putImageData(imgData, 0, 0);
        // Draw resized to mapCanvas for consistent resolution
        mapCtx.clearRect(0,0,200,200);
        mapCtx.drawImage(tmp, 0, 0, 200, 200);
        const mapImage = mapCtx.getImageData(0,0,200,200).data;
        // Convert grayscale to float depth [0,1]
        depthData = new Float32Array(200*200);
        for (let i = 0; i < 200*200; i++) {
          depthData[i] = mapImage[i*4] / 255;
        }
      } else {
        showError('Depth model unavailable. Static reconstruction will be limited.');
        // Fallback: use luminance of uploaded image as pseudo-depth
        const imageData = imgCtx.getImageData(0,0,200,200).data;
        depthData = new Float32Array(200*200);
        for (let i = 0; i < 200*200; i++) {
          const r = imageData[i*4], g = imageData[i*4+1], b = imageData[i*4+2];
          depthData[i] = (0.2126*r + 0.7152*g + 0.0722*b) / 255;
        }
        mapCtx.clearRect(0,0,200,200);
        const dimg = mapCtx.createImageData(200,200);
        for (let i = 0; i < 200*200; i++) {
          const c = Math.round(depthData[i]*255);
          dimg.data[i*4] = c; dimg.data[i*4+1] = c; dimg.data[i*4+2] = c; dimg.data[i*4+3] = 255;
        }
        mapCtx.putImageData(dimg, 0, 0);
      }
    } catch (err) {
      console.error('Depth estimation error:', err);
      showError('Depth estimation failed.');
      return;
    }

    // Compute normals from depth and preview them
    const normalsRGBA = computeNormalsFromDepth(depthData, 200, 200);
    const normalsImage = new ImageData(normalsRGBA, 200, 200);
    mapCtx.putImageData(normalsImage, 0, 0);

    // Reconstruct static points
    const positions = reconstructStaticPoints(depthData, 200, 200, normalsRGBA);

    // Update static geometry
    if (staticGeo) staticGeo.dispose();
    staticGeo = new THREE.BufferGeometry();
    staticGeo.setAttribute('position', new THREE.BufferAttribute(positions, 3));
    staticPoints.geometry = staticGeo;
    staticPoints.visible = true;
    staticEnabled = true;
    toggleStaticBtn.textContent = 'STATIC: ON';
    statusElement.textContent = 'DA3: STATIC READY';
  };
  img.onerror = () => showError('Image failed to load.');
  img.crossOrigin = 'anonymous';
  img.src = url;
});

// Detection loop
let lastDetectionTime = 0;
const DETECTION_INTERVAL = 100; // ms
async function loop(timestamp) {
  if (!detectionInProgress && (timestamp - lastDetectionTime) > DETECTION_INTERVAL) {
    lastDetectionTime = timestamp;
    await processFaceDetection();
  }
  if (needsBufferUpdate) {
    pointsGeo.attributes.position.needsUpdate = true;
    needsBufferUpdate = false;
  }
  renderer.render(scene, camera);
  requestAnimationFrame(loop);
}

window.addEventListener('resize', () => {
  camera.aspect = innerWidth/innerHeight;
  camera.updateProjectionMatrix();
  renderer.setSize(innerWidth, innerHeight);
});

// Initialize the app
processedCanvas = document.createElement('canvas');
processedCanvas.width = 200;
processedCanvas.height = 200;
processedCtx = processedCanvas.getContext('2d', { willReadFrequently: true });
processedCtx.imageSmoothingEnabled = false;

// Permissions-based auto-start
async function checkCameraPermissions() {
  try {
    const permission = await navigator.permissions.query({ name: 'camera' });
    if (permission.state === 'granted') {
      setTimeout(() => { syncButton.click(); }, 500);
    }
  } catch (error) {
    console.log('Permissions API not supported, waiting for user interaction');
  }
}
checkCameraPermissions();
</script>
</body>
</html>
