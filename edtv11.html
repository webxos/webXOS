<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>edTV: CHANNEL 11</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
<style>
  body { margin:0; overflow:hidden; background:#000; font-family:monospace; color:#0f0; }
  #container { position:relative; width:100vw; height:100dvh; }
  video, canvas { position:absolute; top:0; left:0; width:100%; height:100%; object-fit:cover; }
  canvas { transform:scaleX(-1); }           /* mirror selfie view */
  video  { transform:scaleX(-1); }
  #ui {
    position:absolute; bottom:16px; left:50%; transform:translateX(-50%);
    background:rgba(0,255,0,0.1); border:2px solid #0f0; padding:12px 20px;
    border-radius:8px; font-size:14px; backdrop-filter:blur(4px);
  }
  button { background:#000; color:#0f0; border:2px solid #0f0; padding:8px 16px; margin:0 8px; cursor:pointer; }
  button:active { background:#0f0; color:#000; }
</style>
</head>
<body>

<div id="container">
  <video id="video" autoplay playsinline muted></video>
  <canvas id="canvas"></canvas>
  <div id="ui">
    <button id="start">START CHANNEL 11</button>
    <span id="status">Click START → Allow Camera</span>
  </div>
</div>

<!-- Libraries -->
<script src="https://cdn.jsdelivr.net/npm/three@0.168.0/build/three.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>

<script>
// Core elements
const video = document.getElementById('video');
const canvas = document.getElementById('canvas');
const status = document.getElementById('status');
const ctx = canvas.getContext('webgl');

// Three.js setup
let scene, camera, renderer, mask;
function initThree() {
  scene = new THREE.Scene();
  camera = new THREE.PerspectiveCamera(60, innerWidth/innerHeight, 0.1, 100);
  camera.position.z = 3;

  renderer = new THREE.WebGLRenderer({ canvas, alpha: true, antialias: false });
  renderer.setPixelRatio(Math.min(window.devicePixelRatio, 2));
  renderer.setSize(innerWidth, innerHeight);

  const light = new THREE.HemisphereLight(0xffffff, 0x00ff00, 1.2);
  scene.add(light);

  // Simple red tactical mask (customize as needed)
  const geo = new THREE.SphereGeometry(0.8, 32, 16, 0, Math.PI*2, 0, Math.PI/2);
  const mat = new THREE.MeshPhongMaterial({ color: 0xff0033, transparent: true, opacity: 0.9 });
  mask = new THREE.Mesh(geo, mat);
  mask.rotation.x = Math.PI;
  mask.position.z = 0.4;
  scene.add(mask);

  window.addEventListener('resize', () => {
    camera.aspect = innerWidth/innerHeight;
    camera.updateProjectionMatrix();
    renderer.setSize(innerWidth, innerHeight);
  });
}

// FaceMesh setup
let faceMesh;
function onResults(results) {
  if (!results.multiFaceLandmarks?.[0]) {
    status.textContent = "FACE LOST";
    return;
  }

  const landmarks = results.multiFaceLandmarks[0];
  status.textContent = "FACE LOCKED";

  // Key points
  const nose = landmarks[1];
  const leftEye = landmarks[33];
  const rightEye = landmarks[263];

  // Position
  const scale = 7;
  mask.position.x = (nose.x - 0.5) * scale;
  mask.position.y = -(nose.y - 0.5) * scale * (innerHeight/innerWidth);
  mask.position.z = -nose.z * scale * 2;

  // Scale from eye distance
  const eyeDist = Math.hypot(rightEye.x - leftEye.x, rightEye.y - leftEye.y);
  const dynamicScale = Math.max(0.7, eyeDist * 16);
  mask.scale.set(dynamicScale, dynamicScale, dynamicScale);

  // Rotation
  const midEye = { x: (leftEye.x + rightEye.x)/2, y: (leftEye.y + rightEye.y)/2 };
  mask.rotation.y = (nose.x - midEye.x) * 6;
  mask.rotation.x = (nose.y - midEye.y) * 6;
  mask.rotation.z = Math.atan2(rightEye.y - leftEye.y, rightEye.x - leftEye.x);
}

// Animation loop
function animate() {
  requestAnimationFrame(animate);
  renderer.render(scene, camera);
}

// Start everything
document.getElementById('start').onclick = async () => {
  if (faceMesh) return;

  status.textContent = "Starting camera...";

  try {
    const stream = await navigator.mediaDevices.getUserMedia({
      video: { facingMode: "user" }
    });
    video.srcObject = stream;
    video.play();

    initThree();
    animate();

    faceMesh = new FaceMesh({locateFile: (file) => 
      `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`
    });
    faceMesh.setOptions({
      maxNumFaces: 1,
      refineLandmarks: true,
      minDetectionConfidence: 0.8,
      minTrackingConfidence: 0.8
    });
    faceMesh.onResults(onResults);

    const camera = new Camera(video, {
      onFrame: async () => {
        await faceMesh.send({image: video});
      },
      width: 640,
      height: 480
    });
    camera.start();

    status.textContent = "AR READY – Look at camera";
    document.getElementById('start').style.display = 'none';

  } catch (err) {
    status.textContent = "Camera blocked or unavailable";
    console.error(err);
  }
};
</script>
</body>
</html>
